{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recipe_header"
   },
   "source": [
    "# üß† Ellora Recipe #6: Execution-Aware World Model Thinking LoRA\n",
    "\n",
    "## Problem Statement\n",
    "While existing LLMs can generate code and reason about it, they lack **execution awareness** - understanding how code behaves at runtime, predicting variable states, and comprehending the dynamic world model of program execution. This leads to:\n",
    "\n",
    "- Code that compiles but fails at runtime\n",
    "- Poor debugging capabilities\n",
    "- Inability to predict program behavior\n",
    "- Lack of state-aware code generation\n",
    "\n",
    "## Solution Approach\n",
    "This recipe adapts concepts from Meta's **CWM (Code World Model)** paper to create a LoRA adapter that adds execution awareness to the Qwen3-4B-Thinking-2507 model. We combine:\n",
    "\n",
    "1. **Thinking-Enhanced Data Generation**: Leverage Qwen3's native `<think>` tags\n",
    "2. **Real Execution Traces**: Ground truth from actual Python execution\n",
    "3. **World Model Training**: Understanding program state evolution\n",
    "4. **Multi-Task RL**: GRPO training with execution-based rewards\n",
    "\n",
    "## Key Innovation\n",
    "Unlike traditional approaches that only train on static code, we train on **dynamic execution traces** combined with the model's reasoning process, creating a \"neural debugger\" that understands both the logic AND runtime behavior of code.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "- GPU with 24GB+ VRAM (RTX 4090, A100, H100)\n",
    "- Python 3.9+\n",
    "- CUDA 12.0+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## üõ†Ô∏è Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Install PyTorch 2.8+ (compatible with flash_attn 2.8.x)\n",
    "!pip install -q \"torch>=2.8.0\" torchvision torchaudio\n",
    "!pip install -q transformers>=4.46.0\n",
    "!pip install -q peft>=0.13.0\n",
    "!pip install -q trl>=0.11.0\n",
    "!pip install -q datasets>=3.0.0\n",
    "!pip install -q accelerate>=0.34.0\n",
    "!pip install -q bitsandbytes>=0.44.0\n",
    "!pip install -q wandb\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q numpy scipy scikit-learn\n",
    "!pip install -q tqdm matplotlib seaborn\n",
    "\n",
    "# Install flash-attn from source for PyTorch 2.8+ compatibility\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "imports"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• PyTorch version: 2.8.0+cu128\n",
      "üöÄ CUDA available: True\n",
      "üì± GPU: NVIDIA A40\n",
      "üíæ VRAM: 47.7 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "import types\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, BitsAndBytesConfig,\n",
    "    pipeline, set_seed\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, get_peft_model, TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from trl import (\n",
    "    GRPOTrainer, GRPOConfig,  # Changed from DPO to GRPO\n",
    "    SFTTrainer, SFTConfig\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üì± GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration loaded!\n",
      "Model: Qwen/Qwen3-4B-Thinking-2507\n",
      "Training samples: 5000\n",
      "Max sequence length: 32768\n",
      "Dataset Hub ID: codelion/execution-world-model-dataset\n",
      "Batch Size: 128 (MAXIMUM - tested at 1.99 samples/s)\n",
      "Checkpoint dir: ./checkpoints\n",
      "GRPO generations: 4\n",
      "‚ö° Estimated data generation time: ~2.1 hours (26x faster than original!)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class RecipeConfig:\n",
    "    # Model configuration\n",
    "    model_name: str = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "    max_seq_length: int = 32768  # Use substantial context for execution traces\n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_r: int = 64  # Higher rank for 4B model\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: List[str] = None\n",
    "    \n",
    "    # Data generation\n",
    "    num_train_samples: int = 5000\n",
    "    num_eval_samples: int = 500\n",
    "    max_code_length: int = 500  # Lines of code\n",
    "    \n",
    "    # Optimized transformers inference configuration\n",
    "    batch_size: int = 128  # OPTIMIZED: Maximum tested - 1.99 samples/s, ~2.1 hours for 15k (40% GPU)\n",
    "    max_new_tokens_code: int = 512  # Max tokens for code generation\n",
    "    max_new_tokens_exec: int = 1024  # Max tokens for execution prediction\n",
    "    use_torch_compile: bool = False  # Disabled due to compatibility\n",
    "    \n",
    "    # Checkpointing configuration (NEW)\n",
    "    checkpoint_dir: str = \"./checkpoints\"\n",
    "    checkpoint_interval: int = 50  # Save every N batches\n",
    "    \n",
    "    # Training configuration\n",
    "    per_device_train_batch_size: int = 1\n",
    "    per_device_eval_batch_size: int = 4  # CRITICAL: Must match num_generations for GRPO eval\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_train_epochs: int = 3\n",
    "    learning_rate: float = 2e-5\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # GRPO specific (changed from DPO)\n",
    "    num_generations: int = 4  # Number of generations per prompt\n",
    "    generation_batch_size: int = 4  # Must be divisible by num_generations\n",
    "    temperature: float = 0.7\n",
    "    beta: float = 0.1  # KL divergence coefficient\n",
    "    max_prompt_length: int = 512\n",
    "    max_completion_length: int = 1024\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = \"./results/recipe_6_execution_world_model\"\n",
    "    hub_model_id: str = \"codelion/Qwen3-4B-execution-world-model-lora\"\n",
    "    hub_dataset_id: str = \"codelion/execution-world-model-dataset\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            # Qwen3 specific target modules\n",
    "            self.target_modules = [\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ]\n",
    "        # Create checkpoint directory\n",
    "        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = RecipeConfig()\n",
    "print(\"üìã Configuration loaded!\")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Training samples: {config.num_train_samples}\")\n",
    "print(f\"Max sequence length: {config.max_seq_length}\")\n",
    "print(f\"Dataset Hub ID: {config.hub_dataset_id}\")\n",
    "print(f\"Batch Size: {config.batch_size} (MAXIMUM - tested at 1.99 samples/s)\")\n",
    "print(f\"Checkpoint dir: {config.checkpoint_dir}\")\n",
    "print(f\"GRPO generations: {config.num_generations}\")\n",
    "print(f\"‚ö° Estimated data generation time: ~2.1 hours (26x faster than original!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_gen_section"
   },
   "source": [
    "## üè≠ Data Generation Pipeline\n",
    "\n",
    "Our data generation combines three key components:\n",
    "1. **Thinking-Enhanced Code Generation** (Magpie-style)\n",
    "2. **Real Execution Tracing** (Ground truth)\n",
    "3. **Reward-Based Samples** (For GRPO training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "execution_tracer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Execution tracer test:\n",
      "<execution_trace>\n",
      "Line 2: State: {}\n",
      "Line 3: State: {x=5}\n",
      "Line 4: State: {x=5, y=10}\n",
      "</execution_trace>\n",
      "‚úÖ Tracer working correctly!\n"
     ]
    }
   ],
   "source": [
    "class ExecutionTracer:\n",
    "    \"\"\"Traces Python code execution to capture variable states and program flow.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trace_data = []\n",
    "        self.current_locals = {}\n",
    "        self.call_stack = []\n",
    "        \n",
    "    def trace_function(self, frame, event, arg):\n",
    "        \"\"\"Trace function called by sys.settrace.\"\"\"\n",
    "        if event == 'line':\n",
    "            filename = frame.f_code.co_filename\n",
    "            if '<string>' in filename or 'temp_code' in filename:\n",
    "                lineno = frame.f_lineno\n",
    "                locals_copy = dict(frame.f_locals)\n",
    "                \n",
    "                # Filter out built-ins and modules\n",
    "                filtered_locals = {\n",
    "                    k: v for k, v in locals_copy.items()\n",
    "                    if not k.startswith('__') and \n",
    "                    not isinstance(v, (type, types.ModuleType, types.FunctionType))\n",
    "                }\n",
    "                \n",
    "                self.trace_data.append({\n",
    "                    'line': lineno,\n",
    "                    'locals': filtered_locals.copy(),\n",
    "                    'event': event\n",
    "                })\n",
    "                \n",
    "        elif event == 'call':\n",
    "            func_name = frame.f_code.co_name\n",
    "            self.call_stack.append(func_name)\n",
    "            \n",
    "        elif event == 'return':\n",
    "            if self.call_stack:\n",
    "                self.call_stack.pop()\n",
    "                \n",
    "        return self.trace_function\n",
    "    \n",
    "    def execute_and_trace(self, code: str, test_inputs: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Execute code and return execution trace.\"\"\"\n",
    "        import io\n",
    "        from contextlib import redirect_stdout, redirect_stderr\n",
    "        \n",
    "        self.trace_data = []\n",
    "        self.current_locals = {}\n",
    "        self.call_stack = []\n",
    "        \n",
    "        result = {\n",
    "            'success': False,\n",
    "            'output': None,\n",
    "            'error': None,\n",
    "            'trace': [],\n",
    "            'final_state': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Set up the execution environment\n",
    "            global_ns = {'__builtins__': __builtins__}\n",
    "            if test_inputs:\n",
    "                global_ns.update(test_inputs)\n",
    "            \n",
    "            # Install tracer and execute\n",
    "            sys.settrace(self.trace_function)\n",
    "            \n",
    "            # Compile and execute the code WITH OUTPUT SUPPRESSION\n",
    "            compiled_code = compile(code, '<string>', 'exec')\n",
    "            \n",
    "            # Suppress stdout and stderr to prevent kernel crashes from excessive output\n",
    "            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "                exec(compiled_code, global_ns)\n",
    "            \n",
    "            result['success'] = True\n",
    "            result['trace'] = self.trace_data\n",
    "            result['final_state'] = {\n",
    "                k: v for k, v in global_ns.items() \n",
    "                if not k.startswith('__') and k != '__builtins__'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['error'] = {\n",
    "                'type': type(e).__name__,\n",
    "                'message': str(e),\n",
    "                'traceback': traceback.format_exc()\n",
    "            }\n",
    "            result['trace'] = self.trace_data\n",
    "            \n",
    "        finally:\n",
    "            sys.settrace(None)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def format_trace_for_training(self, trace_result: Dict[str, Any]) -> str:\n",
    "        \"\"\"Format execution trace for training data.\"\"\"\n",
    "        if not trace_result['success'] and not trace_result['trace']:\n",
    "            return f\"<execution_error>{trace_result['error']['message']}</execution_error>\"\n",
    "        \n",
    "        formatted_lines = []\n",
    "        formatted_lines.append(\"<execution_trace>\")\n",
    "        \n",
    "        for step in trace_result['trace']:\n",
    "            line_num = step['line']\n",
    "            locals_state = step['locals']\n",
    "            \n",
    "            if locals_state:\n",
    "                state_str = \", \".join([f\"{k}={repr(v)}\" for k, v in locals_state.items()])\n",
    "                formatted_lines.append(f\"Line {line_num}: State: {{{state_str}}}\")\n",
    "            else:\n",
    "                formatted_lines.append(f\"Line {line_num}: State: {{}}\")\n",
    "        \n",
    "        if trace_result['error']:\n",
    "            formatted_lines.append(f\"Error: {trace_result['error']['message']}\")\n",
    "        \n",
    "        formatted_lines.append(\"</execution_trace>\")\n",
    "        return \"\\n\".join(formatted_lines)\n",
    "\n",
    "\n",
    "# Test the execution tracer\n",
    "tracer = ExecutionTracer()\n",
    "test_code = \"\"\"\n",
    "x = 5\n",
    "y = x * 2\n",
    "z = x + y\n",
    "\"\"\"\n",
    "\n",
    "result = tracer.execute_and_trace(test_code)\n",
    "formatted_trace = tracer.format_trace_for_training(result)\n",
    "print(\"üß™ Execution tracer test:\")\n",
    "print(formatted_trace)\n",
    "print(\"‚úÖ Tracer working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "thinking_magpie_generator"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ OptimizedThinkingMagpieGenerator with quality filtering ready!\n"
     ]
    }
   ],
   "source": [
    "class CodeQualityFilter:\n",
    "    \"\"\"Filter to reject low-quality generated code.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid_code(code: str) -> tuple:\n",
    "        \"\"\"Check if code meets quality standards. Returns (is_valid, reason).\"\"\"\n",
    "        if not code or not code.strip():\n",
    "            return False, \"Empty code\"\n",
    "        \n",
    "        lines = [line for line in code.split('\\n') if line.strip() and not line.strip().startswith('#')]\n",
    "        \n",
    "        # Minimum lines check\n",
    "        if len(lines) < 3:\n",
    "            return False, f\"Too simple: only {len(lines)} lines\"\n",
    "        \n",
    "        # Check for excessive constant prints\n",
    "        constant_prints = code.count('print(0)') + code.count('print(1)') + code.count('print(2)')\n",
    "        if constant_prints > 2:\n",
    "            return False, f\"Too many constant prints: {constant_prints}\"\n",
    "        \n",
    "        # Infinite loop check\n",
    "        if 'while True:' in code and 'break' not in code:\n",
    "            return False, \"Potential infinite loop\"\n",
    "        \n",
    "        # Very long iterations check\n",
    "        range_matches = re.findall(r'range\\((\\d+)\\)', code)\n",
    "        for match in range_matches:\n",
    "            if int(match) > 1000:\n",
    "                return False, f\"Too many iterations: range({match})\"\n",
    "        \n",
    "        # Simple print-only loop check\n",
    "        if re.search(r'for .* in .*:\\s*print\\([^)]*\\)\\s*$', code, re.MULTILINE):\n",
    "            return False, \"Loop that only prints\"\n",
    "        \n",
    "        # Meaningful operations check\n",
    "        has_operators = any(op in code for op in ['+', '-', '*', '/', '==', '!=', 'append', 'return', '.extend', '.update'])\n",
    "        if not has_operators:\n",
    "            return False, \"No meaningful operations\"\n",
    "        \n",
    "        # Variable assignments check (at least 2)\n",
    "        assignment_count = code.count('=')\n",
    "        comparison_count = code.count('==') + code.count('!=') + code.count('<=') + code.count('>=')\n",
    "        actual_assignments = assignment_count - comparison_count\n",
    "        if actual_assignments < 2:\n",
    "            return False, f\"Too few assignments: {actual_assignments}\"\n",
    "        \n",
    "        return True, \"Passed quality checks\"\n",
    "\n",
    "\n",
    "class OptimizedThinkingMagpieGenerator:\n",
    "    \"\"\"Optimized transformers-based generator with batching, quality filtering, and better prompts.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, config: RecipeConfig):\n",
    "        print(f\"üöÄ Loading model: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer with left padding for batching\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"  # Left padding for batched generation\n",
    "        \n",
    "        # Load model with optimizations\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Apply torch.compile for faster inference (PyTorch 2.0+)\n",
    "        if config.use_torch_compile and hasattr(torch, 'compile'):\n",
    "            print(\"üî• Compiling model with torch.compile()...\")\n",
    "            try:\n",
    "                self.model.forward = torch.compile(\n",
    "                    self.model.forward, \n",
    "                    mode=\"reduce-overhead\",\n",
    "                    fullgraph=False\n",
    "                )\n",
    "                \n",
    "                # Warm up compiled model\n",
    "                print(\"‚ô®Ô∏è Warming up compiled model...\")\n",
    "                dummy_input = self.tokenizer(\"Warm up\", return_tensors=\"pt\").to(self.model.device)\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model.generate(**dummy_input, max_new_tokens=10)\n",
    "                \n",
    "                print(\"‚úÖ Model compilation complete!\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è torch.compile() failed: {e}, continuing without compilation\")\n",
    "        \n",
    "        self.config = config\n",
    "        self.quality_filter = CodeQualityFilter()\n",
    "        \n",
    "        # IMPROVED prompts with explicit ```python``` code block examples\n",
    "        self.code_prompts = [\n",
    "            \"\"\"Write a Python function in a ```python``` code block. Example:\n",
    "\n",
    "```python\n",
    "def calculate_stats(numbers):\n",
    "    total = sum(numbers)\n",
    "    count = len(numbers)\n",
    "    mean = total / count\n",
    "    minimum = min(numbers)\n",
    "    maximum = max(numbers)\n",
    "    return {'mean': mean, 'min': minimum, 'max': maximum}\n",
    "\n",
    "result = calculate_stats([1, 2, 3, 4, 5])\n",
    "```\n",
    "\n",
    "Now write YOUR OWN function with different logic (not statistics). Use at least 5 variables. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write Python code in a ```python``` block to process data:\n",
    "\n",
    "```python\n",
    "def find_duplicates(items):\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    for item in items:\n",
    "        if item in seen:\n",
    "            if item not in duplicates:\n",
    "                duplicates.append(item)\n",
    "        else:\n",
    "            seen.add(item)\n",
    "    return duplicates\n",
    "\n",
    "result = find_duplicates([1, 2, 3, 2, 4, 3, 5])\n",
    "```\n",
    "\n",
    "Create YOUR version processing lists or data differently. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write a validation function in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def validate_input(value):\n",
    "    min_val = 0\n",
    "    max_val = 100\n",
    "    is_valid = min_val <= value <= max_val\n",
    "    if is_valid:\n",
    "        status = \"accepted\"\n",
    "        error_msg = None\n",
    "    else:\n",
    "        status = \"rejected\"\n",
    "        error_msg = \"Out of range\"\n",
    "    return {'status': status, 'error': error_msg, 'value': value}\n",
    "\n",
    "output = validate_input(42)\n",
    "```\n",
    "\n",
    "Write YOUR validation logic with different rules. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write string processing code in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def analyze_text(text):\n",
    "    char_count = len(text)\n",
    "    word_list = text.split()\n",
    "    word_count = len(word_list)\n",
    "    first_word = word_list[0] if word_list else \"\"\n",
    "    last_word = word_list[-1] if word_list else \"\"\n",
    "    return {'chars': char_count, 'words': word_count, 'first': first_word, 'last': last_word}\n",
    "\n",
    "result = analyze_text(\"Hello world from Python\")\n",
    "```\n",
    "\n",
    "Create YOUR text analysis with different operations. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write dictionary processing code in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def merge_data(dict1, dict2):\n",
    "    result = {}\n",
    "    for key in dict1:\n",
    "        result[key] = dict1[key]\n",
    "    for key in dict2:\n",
    "        if key in result:\n",
    "            result[key] = result[key] + dict2[key]\n",
    "        else:\n",
    "            result[key] = dict2[key]\n",
    "    return result\n",
    "\n",
    "merged = merge_data({'a': 1, 'b': 2}, {'b': 3, 'c': 4})\n",
    "```\n",
    "\n",
    "Write YOUR dictionary operations with different logic. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write a search function in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def find_in_list(items, target):\n",
    "    position = -1\n",
    "    found = False\n",
    "    attempts = 0\n",
    "    for i in range(len(items)):\n",
    "        attempts += 1\n",
    "        if items[i] == target:\n",
    "            position = i\n",
    "            found = True\n",
    "            break\n",
    "    return {'found': found, 'position': position, 'attempts': attempts}\n",
    "\n",
    "result = find_in_list([10, 20, 30, 40], 30)\n",
    "```\n",
    "\n",
    "Create YOUR search algorithm. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write mathematical code in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def calculate_series(n):\n",
    "    total = 0\n",
    "    current = 1\n",
    "    for i in range(n):\n",
    "        total = total + current\n",
    "        current = current * 2\n",
    "    average = total / n\n",
    "    return {'total': total, 'average': average, 'final': current}\n",
    "\n",
    "result = calculate_series(5)\n",
    "```\n",
    "\n",
    "Write YOUR math calculations with different formula. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write list processing code in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def process_nested(nested_list):\n",
    "    flat = []\n",
    "    count = 0\n",
    "    for sublist in nested_list:\n",
    "        for item in sublist:\n",
    "            flat.append(item)\n",
    "            count += 1\n",
    "    total = sum(flat)\n",
    "    return {'flat': flat, 'count': count, 'sum': total}\n",
    "\n",
    "result = process_nested([[1, 2], [3, 4], [5, 6]])\n",
    "```\n",
    "\n",
    "Write YOUR list processing logic. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write error handling code in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def safe_operation(a, b):\n",
    "    result = None\n",
    "    error = None\n",
    "    status = \"\"\n",
    "    try:\n",
    "        result = a / b\n",
    "        status = \"success\"\n",
    "    except ZeroDivisionError:\n",
    "        error = \"Division by zero\"\n",
    "        status = \"error\"\n",
    "    return {'result': result, 'status': status, 'error': error}\n",
    "\n",
    "output = safe_operation(10, 2)\n",
    "```\n",
    "\n",
    "Write YOUR error handling code. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write data transformation code in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def transform_data(numbers):\n",
    "    doubled = []\n",
    "    evens = []\n",
    "    for num in numbers:\n",
    "        doubled.append(num * 2)\n",
    "        if num % 2 == 0:\n",
    "            evens.append(num)\n",
    "    total_doubled = sum(doubled)\n",
    "    return {'doubled': doubled, 'evens': evens, 'sum': total_doubled}\n",
    "\n",
    "result = transform_data([1, 2, 3, 4, 5])\n",
    "```\n",
    "\n",
    "Write YOUR data transformation. Must use ```python``` block.\"\"\"\n",
    "        ]\n",
    "        \n",
    "        # Debugging scenarios with explicit ```python``` code blocks\n",
    "        self.debug_prompts = [\n",
    "            \"\"\"Write Python code with a bug in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def access_list(items):\n",
    "    last_index = len(items)  # Bug: should be len(items) - 1\n",
    "    last_item = items[last_index]  # IndexError!\n",
    "    return last_item\n",
    "\n",
    "result = access_list([1, 2, 3])\n",
    "```\n",
    "\n",
    "Create YOUR buggy code. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write code that fails on empty input in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def get_first(data):\n",
    "    first = data[0]  # Bug: no check for empty list!\n",
    "    result = first * 2\n",
    "    return result\n",
    "\n",
    "output = get_first([1, 2, 3])\n",
    "```\n",
    "\n",
    "Create YOUR code with edge case bug. Must use ```python``` block.\"\"\",\n",
    "\n",
    "            \"\"\"Write recursive code in a ```python``` block:\n",
    "\n",
    "```python\n",
    "def factorial(n):\n",
    "    # Bug: missing base case!\n",
    "    result = n * factorial(n - 1)\n",
    "    return result\n",
    "\n",
    "value = factorial(5)\n",
    "```\n",
    "\n",
    "Create YOUR recursive function. Must use ```python``` block.\"\"\",\n",
    "        ]\n",
    "        \n",
    "        print(f\"‚úÖ Optimized generator ready with quality filtering!\")\n",
    "    \n",
    "    def _format_chat_prompt(self, user_message: str) -> str:\n",
    "        \"\"\"Format message using chat template.\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    \n",
    "    def generate_batch(self, prompts: List[str], max_new_tokens: int) -> List[str]:\n",
    "        \"\"\"Generate responses for a batch of prompts with optimizations.\"\"\"\n",
    "        # Format all prompts with chat template\n",
    "        formatted_prompts = [self._format_chat_prompt(p) for p in prompts]\n",
    "        \n",
    "        # Tokenize with left padding for batching\n",
    "        inputs = self.tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=4096\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Batch generation with optimizations\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=True,  # Enable KV caching\n",
    "                num_beams=1,  # Greedy for speed\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated tokens (skip input)\n",
    "        responses = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            # Find where the actual input ends\n",
    "            input_len = inputs.input_ids[i].shape[0]\n",
    "            generated_tokens = output[input_len:]\n",
    "            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            responses.append(response.strip())\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def fix_indentation(self, code: str) -> str:\n",
    "        \"\"\"Fix common indentation problems - IMPROVED VERSION.\"\"\"\n",
    "        if not code.strip():\n",
    "            return code\n",
    "        \n",
    "        lines = code.split('\\n')\n",
    "        while lines and not lines[0].strip():\n",
    "            lines.pop(0)\n",
    "        while lines and not lines[-1].strip():\n",
    "            lines.pop()\n",
    "        \n",
    "        if not lines:\n",
    "            return \"\"\n",
    "        \n",
    "        min_indent = float('inf')\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                indent = len(line) - len(line.lstrip())\n",
    "                min_indent = min(min_indent, indent)\n",
    "        \n",
    "        if min_indent == float('inf'):\n",
    "            return code\n",
    "        \n",
    "        dedented_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                dedented_lines.append(line[min_indent:])\n",
    "            else:\n",
    "                dedented_lines.append('')\n",
    "        \n",
    "        return '\\n'.join(dedented_lines)\n",
    "    \n",
    "    def extract_code_from_response(self, response: str) -> Optional[str]:\n",
    "        \"\"\"Extract executable Python code with proper indentation - IMPROVED VERSION.\"\"\"\n",
    "        code_block_pattern = r'```python\\s*\\n(.*?)\\n```'\n",
    "        matches = re.findall(code_block_pattern, response, re.DOTALL)\n",
    "        \n",
    "        if matches:\n",
    "            code = matches[0].strip()\n",
    "            code = self.fix_indentation(code)\n",
    "            return code\n",
    "        \n",
    "        # Fallback to line-by-line extraction\n",
    "        lines = response.split('\\n')\n",
    "        code_lines = []\n",
    "        in_code = False\n",
    "        code_starters = ('def ', 'class ', 'import ', 'from ', '@')\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if not stripped and not in_code:\n",
    "                continue\n",
    "            \n",
    "            if stripped.startswith(code_starters):\n",
    "                in_code = True\n",
    "                code_lines.append(line)\n",
    "            elif in_code:\n",
    "                is_code_line = (\n",
    "                    line.startswith((' ', '\\t'))\n",
    "                    or stripped.startswith(('return', 'if ', 'else:', 'elif ', 'for ', 'while ', 'try:', 'except', 'finally:', 'with ', 'assert'))\n",
    "                    or '=' in stripped\n",
    "                    or stripped.endswith(':')\n",
    "                    or not stripped\n",
    "                    or stripped.startswith(('#', '\"\"\"', \"'''\"))\n",
    "                )\n",
    "                \n",
    "                if stripped and not is_code_line:\n",
    "                    words = stripped.split()\n",
    "                    if len(words) > 3 and not stripped.startswith(code_starters):\n",
    "                        break\n",
    "                \n",
    "                code_lines.append(line)\n",
    "        \n",
    "        if code_lines:\n",
    "            code = '\\n'.join(code_lines)\n",
    "            code = self.fix_indentation(code)\n",
    "            if any(starter in code for starter in code_starters):\n",
    "                return code\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def generate_code_scenarios(self, num_samples: int, batch_size: int = 32) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate multiple code scenarios with batched inference and quality filtering.\"\"\"\n",
    "        scenarios = []\n",
    "        rejected_count = 0\n",
    "        rejection_reasons = {}\n",
    "        \n",
    "        # Generate all prompts upfront\n",
    "        all_prompts = []\n",
    "        for _ in range(num_samples):\n",
    "            # Mix of code generation and debugging prompts\n",
    "            if random.random() < 0.8:  # 80% generation, 20% debugging\n",
    "                prompt = random.choice(self.code_prompts)\n",
    "            else:\n",
    "                prompt = random.choice(self.debug_prompts)\n",
    "            all_prompts.append(prompt)\n",
    "        \n",
    "        # Process in batches\n",
    "        num_batches = (len(all_prompts) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in tqdm(range(num_batches), desc=\"Generating code scenarios (batched)\"):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, len(all_prompts))\n",
    "            batch_prompts = all_prompts[start_idx:end_idx]\n",
    "            \n",
    "            try:\n",
    "                # Batch generation\n",
    "                responses = self.generate_batch(batch_prompts, self.config.max_new_tokens_code)\n",
    "                \n",
    "                # Process responses with quality filtering\n",
    "                for prompt, response in zip(batch_prompts, responses):\n",
    "                    extracted_code = self.extract_code_from_response(response)\n",
    "                    \n",
    "                    if not extracted_code:\n",
    "                        rejected_count += 1\n",
    "                        rejection_reasons['No code extracted'] = rejection_reasons.get('No code extracted', 0) + 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Check code length\n",
    "                    if len(extracted_code.split('\\n')) > self.config.max_code_length:\n",
    "                        rejected_count += 1\n",
    "                        rejection_reasons['Too long'] = rejection_reasons.get('Too long', 0) + 1\n",
    "                        continue\n",
    "                    \n",
    "                    # QUALITY FILTER - reject low-quality code\n",
    "                    is_valid, reason = self.quality_filter.is_valid_code(extracted_code)\n",
    "                    if not is_valid:\n",
    "                        rejected_count += 1\n",
    "                        rejection_reasons[reason] = rejection_reasons.get(reason, 0) + 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Passed all checks - accept this code\n",
    "                    scenarios.append({\n",
    "                        'prompt': prompt,\n",
    "                        'response': response,\n",
    "                        'code': extracted_code\n",
    "                    })\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Print quality statistics\n",
    "        total_attempted = len(all_prompts)\n",
    "        accepted = len(scenarios)\n",
    "        print(f\"\\nüìä Code Quality Statistics:\")\n",
    "        print(f\"   Total attempts: {total_attempted}\")\n",
    "        print(f\"   ‚úÖ Accepted: {accepted} ({accepted/total_attempted*100:.1f}%)\")\n",
    "        print(f\"   ‚ùå Rejected: {rejected_count} ({rejected_count/total_attempted*100:.1f}%)\")\n",
    "        if rejection_reasons:\n",
    "            print(f\"   Top rejection reasons:\")\n",
    "            for reason, count in sorted(rejection_reasons.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                print(f\"      - {reason}: {count}\")\n",
    "        \n",
    "        return scenarios\n",
    "\n",
    "print(\"üé≠ OptimizedThinkingMagpieGenerator with quality filtering ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "world_model_data_generator"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç ExecutionWorldModelDataGenerator (GRPO) ready with checkpointing!\n"
     ]
    }
   ],
   "source": [
    "class ExecutionWorldModelDataGenerator:\n",
    "    \"\"\"GRPO data generator - creates prompts with metadata for runtime scoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, config: RecipeConfig):\n",
    "        self.magpie_generator = OptimizedThinkingMagpieGenerator(model_name, config)\n",
    "        self.execution_tracer = ExecutionTracer()\n",
    "        self.config = config\n",
    "    \n",
    "    def generate_training_dataset(self, num_samples: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate GRPO dataset: prompts with execution metadata (no pre-scored responses) with checkpointing.\"\"\"\n",
    "        import glob\n",
    "        import time\n",
    "        \n",
    "        print(f\"üèóÔ∏è Generating {num_samples} GRPO training prompts...\")\n",
    "        \n",
    "        # Setup checkpointing\n",
    "        checkpoint_dir = Path(self.config.checkpoint_dir) / \"grpo_samples\"\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Check for existing checkpoints\n",
    "        checkpoint_file = checkpoint_dir / \"grpo_samples_checkpoint.pkl\"\n",
    "        \n",
    "        grpo_samples = []\n",
    "        execution_errors = 0\n",
    "        successful_traces = []\n",
    "        scenarios = []\n",
    "        scenarios_start_idx = 0\n",
    "        \n",
    "        # Resume from checkpoint if exists\n",
    "        if checkpoint_file.exists():\n",
    "            print(f\"üì• Found existing checkpoint\")\n",
    "            try:\n",
    "                with open(checkpoint_file, 'rb') as f:\n",
    "                    checkpoint_data = pickle.load(f)\n",
    "                \n",
    "                grpo_samples = checkpoint_data['grpo_samples']\n",
    "                execution_errors = checkpoint_data['execution_errors']\n",
    "                successful_traces = checkpoint_data['successful_traces']\n",
    "                scenarios = checkpoint_data.get('scenarios', [])\n",
    "                scenarios_start_idx = checkpoint_data.get('scenarios_processed', 0)\n",
    "                \n",
    "                print(f\"‚úÖ Resumed: {len(grpo_samples)}/{num_samples} samples already generated\")\n",
    "                \n",
    "                # If we already have enough samples, return them\n",
    "                if len(grpo_samples) >= num_samples:\n",
    "                    print(f\"‚úÖ Using {len(grpo_samples)} samples from checkpoint\")\n",
    "                    return grpo_samples[:num_samples]\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load checkpoint: {e}\")\n",
    "                print(\"   Starting from scratch...\")\n",
    "                grpo_samples = []\n",
    "                execution_errors = 0\n",
    "                successful_traces = []\n",
    "                scenarios = []\n",
    "                scenarios_start_idx = 0\n",
    "        \n",
    "        # Step 1: Generate code scenarios (or use cached ones)\n",
    "        if not scenarios or scenarios_start_idx >= len(scenarios):\n",
    "            print(f\"üîß Generating code scenarios...\")\n",
    "            # OPTIMIZED: Reduced from 5x to 3x for faster generation\n",
    "            scenarios = self.magpie_generator.generate_code_scenarios(\n",
    "                num_samples * 3,  # OPTIMIZED: Generate 3x instead of 5x\n",
    "                batch_size=self.config.batch_size\n",
    "            )\n",
    "            print(f\"‚úÖ Generated {len(scenarios)} code scenarios\")\n",
    "            scenarios_start_idx = 0\n",
    "            \n",
    "            # Save scenarios checkpoint\n",
    "            with open(checkpoint_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'grpo_samples': grpo_samples,\n",
    "                    'execution_errors': execution_errors,\n",
    "                    'successful_traces': successful_traces,\n",
    "                    'scenarios': scenarios,\n",
    "                    'scenarios_processed': 0,\n",
    "                    'timestamp': time.time()\n",
    "                }, f)\n",
    "            print(f\"üíæ Scenarios checkpoint saved\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Using {len(scenarios)} cached scenarios (continuing from {scenarios_start_idx})\")\n",
    "        \n",
    "        # Step 2: Create GRPO prompts with execution metadata\n",
    "        checkpoint_interval = 100  # Save every 100 samples\n",
    "        \n",
    "        for idx, scenario in enumerate(tqdm(scenarios[scenarios_start_idx:], \n",
    "                                            desc=\"Creating GRPO prompts with validation\",\n",
    "                                            initial=scenarios_start_idx,\n",
    "                                            total=len(scenarios))):\n",
    "            try:\n",
    "                code = scenario['code']\n",
    "                \n",
    "                # EXECUTION VALIDATION - only keep code that actually runs\n",
    "                exec_result = self.execution_tracer.execute_and_trace(code)\n",
    "                if not exec_result['success']:\n",
    "                    execution_errors += 1\n",
    "                    continue  # Skip code that doesn't execute\n",
    "                \n",
    "                # Store actual trace for reward calculation during training\n",
    "                actual_trace = self.execution_tracer.format_trace_for_training(exec_result)\n",
    "                \n",
    "                # Count trace complexity (more variables = better learning signal)\n",
    "                trace_complexity = len(re.findall(r'State: \\{([^}]*)\\}', actual_trace))\n",
    "                if trace_complexity < 2:\n",
    "                    # Skip traces that are too simple (no learning value)\n",
    "                    continue\n",
    "                \n",
    "                successful_traces.append(trace_complexity)\n",
    "                \n",
    "                # Create GRPO prompt (NO pre-generated response!)\n",
    "                # GRPO will generate responses during training\n",
    "                grpo_sample = {\n",
    "                    'prompt': f\"Analyze this code and predict its execution trace step by step:\\n\\n```python\\n{code}\\n```\\n\\nProvide a detailed execution trace showing variable states at each line.\",\n",
    "                    'code': code,  # Store code for reward function\n",
    "                    'actual_trace': actual_trace,  # Store ground truth for reward function\n",
    "                }\n",
    "                \n",
    "                grpo_samples.append(grpo_sample)\n",
    "                \n",
    "                # CHECKPOINT: Save progress periodically\n",
    "                if len(grpo_samples) % checkpoint_interval == 0:\n",
    "                    with open(checkpoint_file, 'wb') as f:\n",
    "                        pickle.dump({\n",
    "                            'grpo_samples': grpo_samples,\n",
    "                            'execution_errors': execution_errors,\n",
    "                            'successful_traces': successful_traces,\n",
    "                            'scenarios': scenarios,\n",
    "                            'scenarios_processed': scenarios_start_idx + idx + 1,\n",
    "                            'timestamp': time.time()\n",
    "                        }, f)\n",
    "                    print(f\"\\nüíæ Checkpoint: {len(grpo_samples)}/{num_samples} samples saved\")\n",
    "                \n",
    "                # Stop when we have enough samples\n",
    "                if len(grpo_samples) >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating sample: {e}\")\n",
    "                # Save emergency checkpoint\n",
    "                try:\n",
    "                    emergency_file = checkpoint_dir / f\"emergency_checkpoint_{time.time()}.pkl\"\n",
    "                    with open(emergency_file, 'wb') as f:\n",
    "                        pickle.dump({\n",
    "                            'grpo_samples': grpo_samples,\n",
    "                            'execution_errors': execution_errors,\n",
    "                            'successful_traces': successful_traces,\n",
    "                            'scenarios': scenarios,\n",
    "                            'scenarios_processed': scenarios_start_idx + idx,\n",
    "                            'error': str(e)\n",
    "                        }, f)\n",
    "                    print(f\"üíæ Emergency checkpoint saved\")\n",
    "                except:\n",
    "                    pass\n",
    "                continue\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'grpo_samples': grpo_samples,\n",
    "                'execution_errors': execution_errors,\n",
    "                'successful_traces': successful_traces,\n",
    "                'scenarios': scenarios,\n",
    "                'scenarios_processed': len(scenarios),\n",
    "                'completed': True,\n",
    "                'timestamp': time.time()\n",
    "            }, f)\n",
    "        \n",
    "        # Print quality statistics\n",
    "        print(f\"\\nüìä GRPO Data Quality Statistics:\")\n",
    "        print(f\"   ‚úÖ Valid prompts: {len(grpo_samples)}\")\n",
    "        print(f\"   ‚ùå Execution errors rejected: {execution_errors}\")\n",
    "        total_attempts = len(grpo_samples) + execution_errors\n",
    "        if total_attempts > 0:\n",
    "            print(f\"   Success rate: {len(grpo_samples)/total_attempts*100:.1f}%\")\n",
    "        \n",
    "        if successful_traces:\n",
    "            print(f\"\\n   Trace complexity (states per sample):\")\n",
    "            print(f\"   Mean: {np.mean(successful_traces):.1f}\")\n",
    "            print(f\"   Min:  {min(successful_traces)}\")\n",
    "            print(f\"   Max:  {max(successful_traces)}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Generated {len(grpo_samples)} high-quality GRPO prompts\")\n",
    "        print(f\"üí° GRPO will generate and score responses during training\")\n",
    "        print(f\"üíæ Checkpoint saved to: {checkpoint_file}\")\n",
    "        \n",
    "        return grpo_samples\n",
    "\n",
    "print(\"üåç ExecutionWorldModelDataGenerator (GRPO) ready with checkpointing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate_data_section"
   },
   "source": [
    "## üìä Generate Training Data\n",
    "\n",
    "**Note**: This notebook will check if the dataset already exists on HuggingFace Hub. If it does, it will download and use the existing dataset, saving time and computational resources. If not, it will generate a new dataset and push it to the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Please login to HuggingFace Hub to enable dataset push/pull\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc44ebb1d704cbcb28c5a3ef79298ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login to HuggingFace Hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "print(\"üîê Please login to HuggingFace Hub to enable dataset push/pull\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "generate_training_data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing data generator...\n",
      "‚úÖ Found existing dataset on HF Hub: codelion/execution-world-model-dataset\n",
      "üì• Loading dataset from HF Hub: codelion/execution-world-model-dataset\n",
      "‚úÖ Loaded training data: 298 samples\n",
      "‚úÖ Loaded evaluation data: 323 samples\n",
      "\n",
      "üîç Example GRPO training sample:\n",
      "Prompt: Analyze this code and predict its execution trace step by step:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "\n",
      "value = factorial(5)\n",
      "```\n",
      "\n",
      "Pr...\n",
      "Code: def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "\n",
      "value = factorial(5)...\n",
      "Actual trace: <execution_trace>\n",
      "Line 1: State: {}\n",
      "Line 7: State: {}\n",
      "Line 2: State: {n=5}\n",
      "Line 5: State: {n=5}\n",
      "Line 2: State: {n=4}\n",
      "Line 5: State: {n=4}\n",
      "Line 2: State: {n=3}\n",
      "Line 5: State: {n=3}\n",
      "Line 2: State: {n=2}...\n"
     ]
    }
   ],
   "source": [
    "# Initialize data generator\n",
    "print(\"üöÄ Initializing data generator...\")\n",
    "\n",
    "# Check if dataset exists on HuggingFace Hub\n",
    "from huggingface_hub import HfApi, hf_hub_download\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_exists = False\n",
    "try:\n",
    "    api = HfApi()\n",
    "    # Check if dataset repo exists\n",
    "    api.dataset_info(config.hub_dataset_id)\n",
    "    dataset_exists = True\n",
    "    print(f\"‚úÖ Found existing dataset on HF Hub: {config.hub_dataset_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"üìä Dataset not found on HF Hub, will generate new dataset\")\n",
    "    dataset_exists = False\n",
    "\n",
    "if dataset_exists:\n",
    "    # Load existing dataset from HF Hub\n",
    "    print(f\"üì• Loading dataset from HF Hub: {config.hub_dataset_id}\")\n",
    "    dataset_dict = load_dataset(config.hub_dataset_id)\n",
    "    train_data = dataset_dict['train'].to_list()\n",
    "    eval_data = dataset_dict['eval'].to_list()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded training data: {len(train_data)} samples\")\n",
    "    print(f\"‚úÖ Loaded evaluation data: {len(eval_data)} samples\")\n",
    "else:\n",
    "    # Generate new dataset with optimized batched transformers inference\n",
    "    data_generator = ExecutionWorldModelDataGenerator(config.model_name, config)\n",
    "    \n",
    "    # Generate training data\n",
    "    print(\"üìä Generating training data...\")\n",
    "    train_data = data_generator.generate_training_dataset(config.num_train_samples)\n",
    "    \n",
    "    # Generate evaluation data (smaller set)\n",
    "    print(\"üìä Generating evaluation data...\")\n",
    "    eval_data = data_generator.generate_training_dataset(config.num_eval_samples)\n",
    "    \n",
    "    print(f\"‚úÖ Training data: {len(train_data)} samples\")\n",
    "    print(f\"‚úÖ Evaluation data: {len(eval_data)} samples\")\n",
    "    \n",
    "    # Free model memory after data generation\n",
    "    print(\"üßπ Cleaning up model memory...\")\n",
    "    del data_generator.magpie_generator.model\n",
    "    del data_generator.magpie_generator\n",
    "    del data_generator\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ Model memory freed for training!\")\n",
    "    \n",
    "    # Convert to HuggingFace datasets\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    eval_dataset = Dataset.from_list(eval_data)\n",
    "    \n",
    "    # Create dataset dict\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset\n",
    "    })\n",
    "    \n",
    "    # Save locally\n",
    "    dataset_dict.save_to_disk(\"./execution_world_model_dataset\")\n",
    "    print(\"üíæ Datasets saved to disk locally\")\n",
    "    \n",
    "    # Push to HuggingFace Hub\n",
    "    try:\n",
    "        print(f\"‚¨ÜÔ∏è Pushing dataset to HF Hub: {config.hub_dataset_id}\")\n",
    "        dataset_dict.push_to_hub(\n",
    "            config.hub_dataset_id,\n",
    "            private=False,\n",
    "            commit_message=\"Upload execution world model training dataset\"\n",
    "        )\n",
    "        print(f\"‚úÖ Dataset successfully pushed to HF Hub: {config.hub_dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not push to HF Hub: {e}\")\n",
    "        print(\"üí° Make sure you're logged in with: huggingface-cli login\")\n",
    "\n",
    "# Show example (GRPO format)\n",
    "if train_data:\n",
    "    print(\"\\nüîç Example GRPO training sample:\")\n",
    "    example = train_data[0]\n",
    "    print(f\"Prompt: {example['prompt'][:200]}...\")\n",
    "    print(f\"Code: {example['code'][:150]}...\")\n",
    "    print(f\"Actual trace: {example['actual_trace'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "prepare_datasets"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Datasets prepared!\n",
      "Train dataset: 298 samples\n",
      "Eval dataset: 323 samples\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have dataset_dict created (either from HF Hub or freshly generated)\n",
    "if 'dataset_dict' not in locals():\n",
    "    # Convert to HuggingFace datasets if not already done\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    eval_dataset = Dataset.from_list(eval_data)\n",
    "    \n",
    "    # Create dataset dict\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset\n",
    "    })\n",
    "\n",
    "print(\"üìö Datasets prepared!\")\n",
    "print(f\"Train dataset: {len(dataset_dict['train'])} samples\")\n",
    "print(f\"Eval dataset: {len(dataset_dict['eval'])} samples\")\n",
    "\n",
    "# Extract individual datasets for training\n",
    "train_dataset = dataset_dict['train']\n",
    "eval_dataset = dataset_dict['eval']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup_section"
   },
   "source": [
    "## ü§ñ Model Setup and LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02af875d934944b0ac0e960dcbada333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Model loaded: Qwen/Qwen3-4B-Thinking-2507\n",
      "üìä Model parameters: 4,022,468,096\n",
      "üíæ Model memory footprint: 3.37 GB\n"
     ]
    }
   ],
   "source": [
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"ü§ñ Model loaded: {config.model_name}\")\n",
    "print(f\"üìä Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"üíæ Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lora_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gradient checkpointing disabled on model\n",
      "trainable params: 132,120,576 || all params: 4,154,588,672 || trainable%: 3.1801\n",
      "üîß LoRA configuration applied!\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    target_modules=config.target_modules,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# FIX: Disable gradient checkpointing on the model itself\n",
    "# prepare_model_for_kbit_training() enables it by default, but it causes CUDA errors with GRPO generation\n",
    "model.config.use_cache = True\n",
    "if hasattr(model, 'gradient_checkpointing_disable'):\n",
    "    model.gradient_checkpointing_disable()\n",
    "print(\"‚úÖ Gradient checkpointing disabled on model\")\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"üîß LoRA configuration applied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## üèãÔ∏è Training with DPO/GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è GRPO Training configuration ready!\n",
      "üöÄ FULL PRODUCTION MODE:\n",
      "   - num_train_epochs=3 (full training)\n",
      "   - eval_strategy='steps' (evaluation enabled)\n",
      "   - eval_steps=100 (evaluate every 100 steps)\n",
      "   - Expected time: ~75 minutes for 111 steps\n",
      "\n",
      "üí° Critical fixes applied and PRESERVED:\n",
      "   - gradient_checkpointing=False (CRITICAL - prevents numerical instability)\n",
      "   - temperature=0.5 (CRITICAL - prevents NaN/inf probabilities)\n",
      "   - Model gradient checkpointing disabled in Cell-16 (CRITICAL)\n",
      "   - Gradual reward function in Cell-19 (prevents loss=0.0)\n",
      "   - top_k=50 (limits sampling space)\n",
      "   - remove_unused_columns=True (handles extra dataset fields)\n",
      "\n",
      "üìä Training parameters:\n",
      "   - num_generations=4\n",
      "   - max_completion_length=1024 (full traces)\n",
      "   - max_prompt_length=256 (fits all prompts)\n",
      "   - Total steps: 111 (298 samples / batch 8 √ó 3 epochs)\n"
     ]
    }
   ],
   "source": [
    "# GRPO training configuration - FULL PRODUCTION\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=config.output_dir,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    \n",
    "    # Full production training\n",
    "    num_train_epochs=3,  # PRODUCTION: Full 3 epochs\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    weight_decay=config.weight_decay,\n",
    "    \n",
    "    # GRPO specific - OPTIMIZED FOR NUMERICAL STABILITY\n",
    "    num_generations=4,\n",
    "    generation_batch_size=4,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=1024,\n",
    "    temperature=0.5,  # CRITICAL: Prevents NaN/inf probabilities\n",
    "    top_k=50,\n",
    "    beta=config.beta,\n",
    "    \n",
    "    # Memory and performance\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=False,  # CRITICAL: Must be False for stability\n",
    "    dataloader_drop_last=True,\n",
    "    \n",
    "    # Logging, evaluation, and saving\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",  # PRODUCTION: Enable evaluation\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=\"none\",  # Change to \"wandb\" if you want W&B logging\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è GRPO Training configuration ready!\")\n",
    "print(\"üöÄ FULL PRODUCTION MODE:\")\n",
    "print(\"   - num_train_epochs=3 (full training)\")\n",
    "print(\"   - eval_strategy='steps' (evaluation enabled)\")\n",
    "print(\"   - eval_steps=100 (evaluate every 100 steps)\")\n",
    "print(\"   - Expected time: ~75 minutes for 111 steps\")\n",
    "print(\"\\nüí° Critical fixes applied and PRESERVED:\")\n",
    "print(\"   - gradient_checkpointing=False (CRITICAL - prevents numerical instability)\")\n",
    "print(\"   - temperature=0.5 (CRITICAL - prevents NaN/inf probabilities)\")\n",
    "print(\"   - Model gradient checkpointing disabled in Cell-16 (CRITICAL)\")\n",
    "print(\"   - Gradual reward function in Cell-19 (prevents loss=0.0)\")\n",
    "print(\"   - top_k=50 (limits sampling space)\")\n",
    "print(\"   - remove_unused_columns=True (handles extra dataset fields)\")\n",
    "print(\"\\nüìä Training parameters:\")\n",
    "print(\"   - num_generations=4\")\n",
    "print(\"   - max_completion_length=1024 (full traces)\")\n",
    "print(\"   - max_prompt_length=256 (fits all prompts)\")\n",
    "print(\"   - Total steps: 111 (298 samples / batch 8 √ó 3 epochs)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è GRPO Trainer initialized!\n",
      "Training samples: 298\n",
      "Evaluation samples: 323\n",
      "Effective batch size: 8\n",
      "Generations per prompt: 4\n",
      "\n",
      "üí° GRPO will:\n",
      "  1. Generate 4 completions per prompt\n",
      "  2. Execute code from prompt to get ground truth\n",
      "  3. Score each completion using GRADUAL reward system (0.0-1.0)\n",
      "  4. Optimize based on relative rewards (GRPO algorithm)\n",
      "\n",
      "üîß Reward system:\n",
      "   Level 1 (0.0-0.2): Basic keywords and structure\n",
      "   Level 2 (0.2-0.4): Line number patterns\n",
      "   Level 3 (0.4-0.6): State patterns\n",
      "   Level 4 (0.6-0.8): Correct format structure\n",
      "   Level 5 (0.8-1.0): Correct variable values\n"
     ]
    }
   ],
   "source": [
    "# Define reward function for GRPO - IMPROVED GRADUAL SCORING\n",
    "def execution_accuracy_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate execution accuracy rewards for GRPO training.\n",
    "    \n",
    "    Uses GRADUAL SCORING to provide learning signal even when\n",
    "    model doesn't know the format yet. This prevents loss=0.0.\n",
    "    \n",
    "    Scoring levels:\n",
    "    - 0.0-0.2: Basic structure (mentions execution, trace, line, state)\n",
    "    - 0.2-0.4: Format attempt (Line X: patterns)\n",
    "    - 0.4-0.6: State mentions (State: { patterns)\n",
    "    - 0.6-0.8: Correct format structure\n",
    "    - 0.8-1.0: Correct variable values\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "    \n",
    "    tracer = ExecutionTracer()\n",
    "    \n",
    "    # Handle prompt replication for multiple generations\n",
    "    num_completions = len(completions)\n",
    "    num_prompts = len(prompts)\n",
    "    \n",
    "    if num_completions > num_prompts:\n",
    "        num_generations = num_completions // num_prompts\n",
    "        prompts_expanded = []\n",
    "        for prompt in prompts:\n",
    "            for _ in range(num_generations):\n",
    "                prompts_expanded.append(prompt)\n",
    "        prompts = prompts_expanded\n",
    "    \n",
    "    rewards = []\n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        try:\n",
    "            # Extract code from prompt\n",
    "            code_match = re.search(r'```python\\s*\\n(.*?)\\n```', prompt, re.DOTALL)\n",
    "            if not code_match:\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            code = code_match.group(1)\n",
    "            \n",
    "            # Execute code to get ground truth trace\n",
    "            exec_result = tracer.execute_and_trace(code)\n",
    "            if not exec_result['success']:\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            actual_trace = tracer.format_trace_for_training(exec_result)\n",
    "            \n",
    "            # GRADUAL SCORING SYSTEM\n",
    "            reward = 0.0\n",
    "            \n",
    "            # Level 1: Basic structure (0.0 ‚Üí 0.2)\n",
    "            completion_lower = completion.lower()\n",
    "            if any(kw in completion_lower for kw in ['execution', 'trace', 'line', 'state']):\n",
    "                reward += 0.1\n",
    "            if '<execution_trace>' in completion or 'execution_trace' in completion_lower:\n",
    "                reward += 0.1\n",
    "            \n",
    "            # Level 2: Format attempt (0.2 ‚Üí 0.4)\n",
    "            line_mentions = len(re.findall(r'[Ll]ine\\s+\\d+', completion))\n",
    "            if line_mentions > 0:\n",
    "                reward += min(0.2, 0.05 * line_mentions)\n",
    "            \n",
    "            # Level 3: State mentions (0.4 ‚Üí 0.6)\n",
    "            state_mentions = len(re.findall(r'[Ss]tate:\\s*\\{', completion))\n",
    "            if state_mentions > 0:\n",
    "                reward += min(0.2, 0.05 * state_mentions)\n",
    "            \n",
    "            # Level 4: Correct format structure (0.6 ‚Üí 0.8)\n",
    "            pred_states = re.findall(r'State:\\s*\\{([^}]*)\\}', completion)\n",
    "            actual_states = re.findall(r'State:\\s*\\{([^}]*)\\}', actual_trace)\n",
    "            \n",
    "            if pred_states and actual_states:\n",
    "                state_ratio = min(len(pred_states), len(actual_states)) / max(len(actual_states), 1)\n",
    "                reward += 0.2 * state_ratio\n",
    "            \n",
    "            # Level 5: Correct variable values (0.8 ‚Üí 1.0)\n",
    "            if pred_states and actual_states:\n",
    "                def parse_state(state_str):\n",
    "                    vars_dict = {}\n",
    "                    if not state_str.strip():\n",
    "                        return vars_dict\n",
    "                    pairs = state_str.split(',')\n",
    "                    for pair in pairs:\n",
    "                        if '=' in pair:\n",
    "                            key, val = pair.split('=', 1)\n",
    "                            vars_dict[key.strip()] = val.strip()\n",
    "                    return vars_dict\n",
    "                \n",
    "                total_matches = 0\n",
    "                total_variables = 0\n",
    "                \n",
    "                for actual_state in actual_states:\n",
    "                    actual_vars = parse_state(actual_state)\n",
    "                    total_variables += len(actual_vars)\n",
    "                    \n",
    "                    best_match = 0\n",
    "                    for pred_state in pred_states:\n",
    "                        pred_vars = parse_state(pred_state)\n",
    "                        matches = sum(1 for var, val in actual_vars.items() \n",
    "                                    if var in pred_vars and pred_vars[var] == val)\n",
    "                        best_match = max(best_match, matches)\n",
    "                    \n",
    "                    total_matches += best_match\n",
    "                \n",
    "                if total_variables > 0:\n",
    "                    accuracy = total_matches / total_variables\n",
    "                    reward += 0.2 * accuracy\n",
    "            \n",
    "            # Cap at 1.0\n",
    "            reward = min(reward, 1.0)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Silently handle errors\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    # Ensure correct count\n",
    "    while len(rewards) < num_completions:\n",
    "        rewards.append(0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Initialize GRPO trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[execution_accuracy_reward],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"üèãÔ∏è GRPO Trainer initialized!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Generations per prompt: {training_args.num_generations}\")\n",
    "print(\"\\nüí° GRPO will:\")\n",
    "print(f\"  1. Generate {training_args.num_generations} completions per prompt\")\n",
    "print(f\"  2. Execute code from prompt to get ground truth\")\n",
    "print(f\"  3. Score each completion using GRADUAL reward system (0.0-1.0)\")\n",
    "print(f\"  4. Optimize based on relative rewards (GRPO algorithm)\")\n",
    "print(\"\\nüîß Reward system:\")\n",
    "print(\"   Level 1 (0.0-0.2): Basic keywords and structure\")\n",
    "print(\"   Level 2 (0.2-0.4): Line number patterns\")\n",
    "print(\"   Level 3 (0.4-0.6): State patterns\")\n",
    "print(\"   Level 4 (0.6-0.8): Correct format structure\")\n",
    "print(\"   Level 5 (0.8-1.0): Correct variable values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "Total training steps: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'top_k': 20, 'top_p': 0.95, 'bos_token_id': 151643}. If this is not desired, please set these values explicitly.\n",
      "Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='447' max='447' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [447/447 72:09:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>6888134595823188049920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.001021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Total training steps: {len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "\n",
    "# Clear cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## üìä Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "evaluation_setup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluator ready!\n"
     ]
    }
   ],
   "source": [
    "class ExecutionWorldModelEvaluator:\n",
    "    \"\"\"Evaluate the trained model on execution prediction tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tracer = ExecutionTracer()\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_length: int = 1024) -> str:\n",
    "        \"\"\"Generate model response for a given prompt.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][len(inputs.input_ids[0]):], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return response.strip()\n",
    "    \n",
    "    def extract_variables_from_text(self, text: str) -> dict:\n",
    "        \"\"\"Extract variable assignments from natural language - FIXED to handle expressions.\"\"\"\n",
    "        variables = {}\n",
    "        \n",
    "        # Pattern 1: Expression with result \"var = expr = result\"\n",
    "        # Matches: y = 10 * 2 = 20 ‚Üí extracts y, 20 (rightmost value)\n",
    "        # This must come FIRST to catch complex expressions before simple assignments\n",
    "        matches = re.findall(r'([a-zA-Z_]\\w*)\\s*=\\s*[^=\\n]*=\\s*(-?\\d+(?:\\.\\d+)?)', text)\n",
    "        for var, val in matches:\n",
    "            if var.lower() not in ['line', 'step']:\n",
    "                variables[var] = val\n",
    "        \n",
    "        # Pattern 2: Simple assignment \"var = number\" (only if not already found)\n",
    "        # Matches: x = 10 ‚Üí extracts x, 10\n",
    "        # Use word boundary or whitespace after number to avoid matching \"x = 10 * 2\"\n",
    "        matches = re.findall(r'([a-zA-Z_]\\w*)\\s*=\\s*(-?\\d+(?:\\.\\d+)?)(?:\\s|,|\\.|;|$)', text)\n",
    "        for var, val in matches:\n",
    "            if var.lower() not in ['line', 'step'] and var not in variables:\n",
    "                variables[var] = val\n",
    "        \n",
    "        # Pattern 3: \"var becomes number\" or \"var is number\"\n",
    "        matches = re.findall(r'([a-zA-Z_]\\w*)\\s+(?:becomes|is)\\s+(-?\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        for var, val in matches:\n",
    "            if var.lower() not in ['line', 'step'] and var not in variables:\n",
    "                variables[var] = val\n",
    "        \n",
    "        # Pattern 4: \"var: number\" (key-value style)\n",
    "        matches = re.findall(r'([a-zA-Z_]\\w*):\\s*(-?\\d+(?:\\.\\d+)?)', text)\n",
    "        for var, val in matches:\n",
    "            if var.lower() not in ['line', 'step'] and var not in variables:\n",
    "                variables[var] = val\n",
    "        \n",
    "        return variables\n",
    "    \n",
    "    def evaluate_execution_prediction(self, test_cases: List[Dict[str, str]]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model's ability to predict execution traces.\"\"\"\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        state_accuracies = []\n",
    "        \n",
    "        for case in tqdm(test_cases, desc=\"Evaluating execution prediction\"):\n",
    "            code = case['code']\n",
    "            \n",
    "            # Get actual execution trace\n",
    "            actual_result = self.tracer.execute_and_trace(code)\n",
    "            actual_trace = self.tracer.format_trace_for_training(actual_result)\n",
    "            \n",
    "            # Get model prediction\n",
    "            prompt = f\"Analyze this code and predict its execution trace step by step:\\n\\n```python\\n{code}\\n```\\n\\nProvide a detailed execution trace showing variable states at each line.\"\n",
    "            predicted_response = self.generate_response(prompt)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = self.calculate_trace_accuracy(predicted_response, actual_trace)\n",
    "            state_accuracies.append(accuracy)\n",
    "            \n",
    "            if accuracy > 0.7:  # Threshold for \"correct\"\n",
    "                correct_predictions += 1\n",
    "            \n",
    "            total_predictions += 1\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': correct_predictions / max(total_predictions, 1),\n",
    "            'mean_state_accuracy': np.mean(state_accuracies),\n",
    "            'std_state_accuracy': np.std(state_accuracies),\n",
    "            'total_cases': total_predictions\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_trace_accuracy(self, predicted: str, actual: str) -> float:\n",
    "        \"\"\"Calculate accuracy of trace prediction - supports both strict format and natural language.\"\"\"\n",
    "        if not predicted or not actual:\n",
    "            return 0.0\n",
    "        \n",
    "        # Try strict format first\n",
    "        predicted_states = re.findall(r'State: \\{([^}]*)\\}', predicted)\n",
    "        actual_states = re.findall(r'State: \\{([^}]*)\\}', actual)\n",
    "        \n",
    "        if not actual_states:\n",
    "            return 0.5  # Neutral score\n",
    "        \n",
    "        # If model used strict format, use original matching\n",
    "        if predicted_states:\n",
    "            correct_matches = 0\n",
    "            for actual_state in actual_states:\n",
    "                for pred_state in predicted_states:\n",
    "                    if self.states_match(actual_state, pred_state):\n",
    "                        correct_matches += 1\n",
    "                        break\n",
    "            return correct_matches / len(actual_states)\n",
    "        \n",
    "        # Otherwise, use flexible natural language parsing\n",
    "        # Extract all variables from the predicted output\n",
    "        pred_vars = self.extract_variables_from_text(predicted)\n",
    "        \n",
    "        # Extract expected final variable values from actual trace\n",
    "        actual_vars = {}\n",
    "        for state in actual_states:\n",
    "            matches = re.findall(r'(\\w+)=([^,}]+)', state)\n",
    "            for var, val in matches:\n",
    "                # Keep the latest value for each variable\n",
    "                actual_vars[var] = val.strip().strip(\"'\\\"\")\n",
    "        \n",
    "        if not actual_vars:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate accuracy: how many variables have correct values\n",
    "        correct = 0\n",
    "        for var, expected_val in actual_vars.items():\n",
    "            if var in pred_vars:\n",
    "                # Normalize values for comparison\n",
    "                pred_val = str(pred_vars[var]).strip().strip(\"'\\\"\")\n",
    "                if pred_val == expected_val:\n",
    "                    correct += 1\n",
    "        \n",
    "        accuracy = correct / len(actual_vars)\n",
    "        return accuracy\n",
    "    \n",
    "    def states_match(self, actual_state: str, predicted_state: str) -> bool:\n",
    "        \"\"\"Check if two state strings represent similar variable assignments.\"\"\"\n",
    "        actual_vars = re.findall(r'(\\w+)=([^,]+)', actual_state)\n",
    "        predicted_vars = re.findall(r'(\\w+)=([^,]+)', predicted_state)\n",
    "        \n",
    "        # Check if at least 50% of variables match\n",
    "        matches = 0\n",
    "        for var, val in actual_vars:\n",
    "            for p_var, p_val in predicted_vars:\n",
    "                if var == p_var and val.strip() == p_val.strip():\n",
    "                    matches += 1\n",
    "                    break\n",
    "        \n",
    "        return matches >= len(actual_vars) * 0.5\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ExecutionWorldModelEvaluator(trainer.model, tokenizer)\n",
    "print(\"üìä Evaluator ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "create_test_cases"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Created 5 test cases for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create test cases for evaluation\n",
    "test_cases = [\n",
    "    {\n",
    "        'name': 'Simple arithmetic',\n",
    "        'code': '''x = 10\n",
    "y = x * 2\n",
    "z = x + y\n",
    "result = z - 5'''\n",
    "    },\n",
    "    {\n",
    "        'name': 'Loop with accumulator',\n",
    "        'code': '''total = 0\n",
    "for i in range(5):\n",
    "    total += i\n",
    "final_result = total * 2'''\n",
    "    },\n",
    "    {\n",
    "        'name': 'Conditional logic',\n",
    "        'code': '''x = 15\n",
    "if x > 10:\n",
    "    result = x * 2\n",
    "else:\n",
    "    result = x + 5\n",
    "final = result + 1'''\n",
    "    },\n",
    "    {\n",
    "        'name': 'List processing',\n",
    "        'code': '''numbers = [1, 2, 3, 4, 5]\n",
    "total = 0\n",
    "for num in numbers:\n",
    "    total += num\n",
    "average = total / len(numbers)'''\n",
    "    },\n",
    "    {\n",
    "        'name': 'Function with return',\n",
    "        'code': '''def calculate(a, b):\n",
    "    return a * b + 10\n",
    "\n",
    "result = calculate(3, 4)\n",
    "final = result + 5'''\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üß™ Created {len(test_cases)} test cases for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "run_evaluation"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating execution prediction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [08:47<00:00, 105.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation Results:\n",
      "Overall Accuracy: 20.00%\n",
      "Mean State Accuracy: 33.33%\n",
      "Standard Deviation: 0.365\n",
      "Total Test Cases: 5\n",
      "\n",
      "======================================================================\n",
      "üîç DETAILED EXAMPLE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìù Test Code:\n",
      "x = 10\n",
      "y = x * 2\n",
      "z = x + y\n",
      "result = z - 5\n",
      "\n",
      "ü§ñ Model Prediction:\n",
      "Use the following format:\n",
      "\n",
      "```\n",
      "Line 1: x = 10\n",
      "Line 2: y = x * 2\n",
      "Line 3: z = x + y\n",
      "Line 4: result = z - 5\n",
      "```\n",
      "\n",
      "For each line, show the current values of all variables.\n",
      "\n",
      "We are given the code:\n",
      "\n",
      "x = 10\n",
      "y = x * 2\n",
      "z = x + y\n",
      "result = z - 5\n",
      "\n",
      "We'll step through each line.\n",
      "\n",
      "Step 1: Line 1: x = 10\n",
      "   - Before this line, x is undefined (or we can consider it as not assigned yet).\n",
      "   - After assignment: x becomes 10.\n",
      "\n",
      "Step 2: Line 2: y = x * 2\n",
      "   - At this point, x is 10.\n",
      "   - So, y = 10 * 2 = 20.\n",
      "\n",
      "Step 3: Line 3: z = x + y\n",
      "   - Now, x is 10 and y is 20.\n",
      "   - So, z = 10 + 20 = 30.\n",
      "\n",
      "Step 4: Line 4: result = z - 5\n",
      "   - Now, z is 30.\n",
      "   - So, result = 30 - 5 = 25.\n",
      "\n",
      "We'll write the trace in the required format.\n",
      "\n",
      "Note: The problem says to show the current values of all variables at each line.\n",
      "\n",
      "Let's write ...\n",
      "\n",
      "üìä Variables Extracted from Prediction:\n",
      "   y = 20\n",
      "   z = 30\n",
      "   result = 25\n",
      "   x = 10\n",
      "\n",
      "‚úÖ Actual Execution Trace:\n",
      "<execution_trace>\n",
      "Line 1: State: {}\n",
      "Line 2: State: {x=10}\n",
      "Line 3: State: {x=10, y=20}\n",
      "Line 4: State: {x=10, y=20, z=30}\n",
      "</execution_trace>\n",
      "\n",
      "üìä Expected Variables:\n",
      "   x = 10\n",
      "   y = 20\n",
      "   z = 30\n",
      "\n",
      "üìà Accuracy for this example: 100.00%\n",
      "\n",
      "üéØ Variable-by-Variable Comparison:\n",
      "   ‚úÖ x: expected=10, predicted=10\n",
      "   ‚úÖ y: expected=20, predicted=20\n",
      "   ‚úÖ z: expected=30, predicted=30\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"üîç Running evaluation...\")\n",
    "evaluation_results = evaluator.evaluate_execution_prediction(test_cases)\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "print(f\"Overall Accuracy: {evaluation_results['accuracy']:.2%}\")\n",
    "print(f\"Mean State Accuracy: {evaluation_results['mean_state_accuracy']:.2%}\")\n",
    "print(f\"Standard Deviation: {evaluation_results['std_state_accuracy']:.3f}\")\n",
    "print(f\"Total Test Cases: {evaluation_results['total_cases']}\")\n",
    "\n",
    "# Show detailed example with debug information\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç DETAILED EXAMPLE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_code = test_cases[0]['code']\n",
    "prompt = f\"Analyze this code and predict its execution trace step by step:\\n\\n```python\\n{test_code}\\n```\\n\\nProvide a detailed execution trace showing variable states at each line.\"\n",
    "\n",
    "prediction = evaluator.generate_response(prompt)\n",
    "\n",
    "print(f\"\\nüìù Test Code:\")\n",
    "print(test_code)\n",
    "\n",
    "print(f\"\\nü§ñ Model Prediction:\")\n",
    "print(prediction[:800] + (\"...\" if len(prediction) > 800 else \"\"))\n",
    "\n",
    "# Extract variables from prediction\n",
    "extracted_vars = evaluator.extract_variables_from_text(prediction)\n",
    "print(f\"\\nüìä Variables Extracted from Prediction:\")\n",
    "if extracted_vars:\n",
    "    for var, val in extracted_vars.items():\n",
    "        print(f\"   {var} = {val}\")\n",
    "else:\n",
    "    print(\"   (None found)\")\n",
    "\n",
    "# Get actual trace for comparison\n",
    "actual_result = evaluator.tracer.execute_and_trace(test_code)\n",
    "actual_trace = evaluator.tracer.format_trace_for_training(actual_result)\n",
    "\n",
    "print(f\"\\n‚úÖ Actual Execution Trace:\")\n",
    "print(actual_trace)\n",
    "\n",
    "# Extract expected variables\n",
    "actual_vars = {}\n",
    "actual_states = re.findall(r'State: \\{([^}]*)\\}', actual_trace)\n",
    "for state in actual_states:\n",
    "    matches = re.findall(r'(\\w+)=([^,}]+)', state)\n",
    "    for var, val in matches:\n",
    "        actual_vars[var] = val.strip().strip(\"'\\\"\")\n",
    "\n",
    "print(f\"\\nüìä Expected Variables:\")\n",
    "if actual_vars:\n",
    "    for var, val in actual_vars.items():\n",
    "        print(f\"   {var} = {val}\")\n",
    "else:\n",
    "    print(\"   (None found)\")\n",
    "\n",
    "# Calculate accuracy for this example\n",
    "accuracy = evaluator.calculate_trace_accuracy(prediction, actual_trace)\n",
    "print(f\"\\nüìà Accuracy for this example: {accuracy:.2%}\")\n",
    "\n",
    "# Show which variables matched\n",
    "print(f\"\\nüéØ Variable-by-Variable Comparison:\")\n",
    "for var, expected_val in actual_vars.items():\n",
    "    if var in extracted_vars:\n",
    "        pred_val = str(extracted_vars[var]).strip()\n",
    "        match = \"‚úÖ\" if pred_val == expected_val else \"‚ùå\"\n",
    "        print(f\"   {match} {var}: expected={expected_val}, predicted={pred_val}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {var}: expected={expected_val}, predicted=(not found)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model_section"
   },
   "source": [
    "## üíæ Save and Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved locally to results/recipe_6_execution_world_model/final_model\n",
      "üìù Training information saved\n",
      "\n",
      "‚¨ÜÔ∏è  Pushing model to HuggingFace Hub: codelion/Qwen3-4B-execution-world-model-lora\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd3dec47c104298992db0542a1e6615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84613cfa0c5e4db2a39dfe908db8f7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a21a4147e54763bde475245a2b7364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...p5uq4dd9i/adapter_model.safetensors:   0%|          | 61.5kB /  529MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-20T08:51:21.516560Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"01K80D1S935N16HH3KFAYRAQ8H\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:207\n",
      "\n",
      "  \u001b[2m2025-10-20T08:51:22.555681Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"01K80D1T9H5APBRX949CPTTTFE\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:207\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021d0a4d36524dfdad1a7c485e59360c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1587e041684c148d13df1cb2e79d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6309040aaa40c587ff618a20c1949b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmp8kn2twlq/tokenizer.json       : 100%|##########| 11.4MB / 11.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model successfully pushed to HuggingFace Hub!\n",
      "üì¶ Model files uploaded (model card will be added in next cell)\n"
     ]
    }
   ],
   "source": [
    "# Save the trained LoRA adapter locally\n",
    "output_dir = Path(config.output_dir) / \"final_model\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save LoRA adapter locally\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved locally to {output_dir}\")\n",
    "\n",
    "# Save training configuration and results\n",
    "config_dict = {\n",
    "    'model_name': config.model_name,\n",
    "    'lora_r': config.lora_r,\n",
    "    'lora_alpha': config.lora_alpha,\n",
    "    'training_samples': len(train_dataset),\n",
    "    'evaluation_results': evaluation_results,\n",
    "    'training_args': {\n",
    "        'num_train_epochs': config.num_train_epochs,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'batch_size': config.per_device_train_batch_size,\n",
    "        'gradient_accumulation_steps': config.gradient_accumulation_steps,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / \"training_info.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(\"üìù Training information saved\")\n",
    "\n",
    "# Push model to HuggingFace Hub (model card will be uploaded separately in next cell)\n",
    "print(f\"\\n‚¨ÜÔ∏è  Pushing model to HuggingFace Hub: {config.hub_model_id}\")\n",
    "try:\n",
    "    trainer.model.push_to_hub(\n",
    "        config.hub_model_id,\n",
    "        commit_message=f\"Upload execution-aware world model LoRA (accuracy: {evaluation_results['mean_state_accuracy']:.1%})\",\n",
    "        private=False,\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    tokenizer.push_to_hub(\n",
    "        config.hub_model_id,\n",
    "        commit_message=\"Upload tokenizer for execution-aware world model LoRA\",\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    print(f\"‚úÖ Model successfully pushed to HuggingFace Hub!\")\n",
    "    print(f\"üì¶ Model files uploaded (model card will be added in next cell)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Could not push model to Hub: {e}\")\n",
    "    print(\"üí° Make sure you're logged in with write access\")\n",
    "    print(f\"   Model is still saved locally at: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "create_model_card"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Model card created locally\n",
      "\n",
      "üìù Uploading model card to HuggingFace Hub...\n",
      "‚úÖ Model card uploaded successfully!\n",
      "üîó Complete model page: https://huggingface.co/codelion/Qwen3-4B-execution-world-model-lora\n"
     ]
    }
   ],
   "source": [
    "# Create and upload model card to HuggingFace Hub\n",
    "model_card_yaml = \"\"\"---\n",
    "base_model: {model_name}\n",
    "tags:\n",
    "- ellora\n",
    "- lora\n",
    "- code-execution\n",
    "- execution-tracing\n",
    "- world-model\n",
    "- cwm\n",
    "- grpo\n",
    "- thinking\n",
    "- code-understanding\n",
    "- peft\n",
    "- qwen\n",
    "library_name: peft\n",
    "license: apache-2.0\n",
    "pipeline_tag: text-generation\n",
    "inference: true\n",
    "model_type: qwen3\n",
    "datasets:\n",
    "- {dataset_id}\n",
    "---\"\"\"\n",
    "\n",
    "model_card_body = \"\"\"\n",
    "# {hub_id}\n",
    "\n",
    "## üåç Execution-Aware World Model LoRA\n",
    "\n",
    "This LoRA adapter adds **execution awareness** capabilities to {model_name}. Inspired by Meta's CWM (Code World Model) research, it enables the model to predict and understand program execution step-by-step.\n",
    "\n",
    "## üéØ Key Features\n",
    "\n",
    "- **Step-by-Step Execution Prediction**: Predicts variable states at each line\n",
    "- **Dynamic World Model**: Understands how code behaves at runtime\n",
    "- **Execution Tracing**: Generates detailed execution traces with variable states\n",
    "- **Debugging Support**: Can identify and explain execution behavior\n",
    "- **GRPO-Trained**: Uses preference learning with real execution feedback\n",
    "\n",
    "## üìä Performance Metrics\n",
    "\n",
    "- **Base Model**: {model_name}\n",
    "- **Training Method**: GRPO (Group Relative Policy Optimization) with Real Execution Traces\n",
    "- **LoRA Rank**: {lora_r}\n",
    "- **LoRA Alpha**: {lora_alpha}\n",
    "- **Training Samples**: {train_samples:,}\n",
    "- **Evaluation Samples**: {eval_samples:,}\n",
    "- **Execution Prediction Accuracy**: {accuracy:.1%}\n",
    "- **Mean State Accuracy**: {mean_accuracy:.1%}\n",
    "\n",
    "## üîß Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"{model_name}\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{model_name}\")\n",
    "\n",
    "# Load execution world model LoRA\n",
    "model = PeftModel.from_pretrained(model, \"{hub_id}\")\n",
    "\n",
    "# Analyze code execution\n",
    "prompt = \\\"\\\"\\\"Analyze this code and predict its execution trace:\n",
    "\n",
    "\\`\\`\\`python\n",
    "x = 10\n",
    "y = x * 2\n",
    "z = x + y\n",
    "\\`\\`\\`\n",
    "\n",
    "Show variable states at each line.\\\"\\\"\\\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## üìà Example Output\n",
    "\n",
    "```\n",
    "<execution_trace>\n",
    "Line 1: State: {{x=10}}\n",
    "Line 2: State: {{x=10, y=20}}\n",
    "Line 3: State: {{x=10, y=20, z=30}}\n",
    "</execution_trace>\n",
    "```\n",
    "\n",
    "## üß™ Training Details\n",
    "\n",
    "- **Method**: GRPO (Group Relative Policy Optimization)\n",
    "- **Data**: Self-generated code with real execution traces\n",
    "- **Epochs**: {epochs}\n",
    "- **Reward**: Gradual scoring (0.0-1.0) based on execution accuracy\n",
    "\n",
    "## üìö Dataset\n",
    "\n",
    "[{dataset_id}](https://huggingface.co/datasets/{dataset_id})\n",
    "\n",
    "- Python code (3-20 lines)\n",
    "- Real execution traces via `sys.settrace()`\n",
    "- Ground truth variable states\n",
    "\n",
    "## üè∑Ô∏è Related\n",
    "\n",
    "- **Dataset**: [{dataset_id}](https://huggingface.co/datasets/{dataset_id})\n",
    "- **Base Model**: [{model_name}](https://huggingface.co/{model_name})\n",
    "- **Project**: [Ellora Recipes](https://github.com/codelion/ellora)\n",
    "\n",
    "---\n",
    "\n",
    "*Part of the [Ellora project](https://github.com/codelion/ellora) - standardized recipes for enhancing LLM capabilities.*\n",
    "\"\"\"\n",
    "\n",
    "# Format model card with actual values\n",
    "model_card_content = model_card_yaml.format(\n",
    "    model_name=config.model_name,\n",
    "    dataset_id=config.hub_dataset_id\n",
    ") + model_card_body.format(\n",
    "    hub_id=config.hub_model_id,\n",
    "    model_name=config.model_name,\n",
    "    lora_r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    train_samples=len(train_dataset),\n",
    "    eval_samples=len(eval_dataset),\n",
    "    accuracy=evaluation_results['accuracy'],\n",
    "    mean_accuracy=evaluation_results['mean_state_accuracy'],\n",
    "    epochs=config.num_train_epochs,\n",
    "    dataset_id=config.hub_dataset_id\n",
    ")\n",
    "\n",
    "# Save model card locally\n",
    "readme_path = output_dir / \"README.md\"\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "print(\"üìÑ Model card created locally\")\n",
    "\n",
    "# Upload model card to HuggingFace Hub\n",
    "try:\n",
    "    from huggingface_hub import upload_file\n",
    "\n",
    "    print(f\"\\nüìù Uploading model card to HuggingFace Hub...\")\n",
    "    upload_file(\n",
    "        path_or_fileobj=str(readme_path),\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=config.hub_model_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Add model card with YAML frontmatter and usage instructions\",\n",
    "        token=True\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Model card uploaded successfully!\")\n",
    "    print(f\"üîó Complete model page: https://huggingface.co/{config.hub_model_id}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Could not upload model card: {e}\")\n",
    "    print(f\"üíæ Model card saved locally: {readme_path}\")\n",
    "    print(f\"üí° Manual upload: https://huggingface.co/{config.hub_model_id}/edit/main/README.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "usage_example"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing the trained model...\n",
      "\n",
      "üß™ Test Code:\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "result = fibonacci(4)\n",
      "print(result)\n",
      "\n",
      "ü§ñ Model Response:\n",
      "Use the following format:\n",
      "\n",
      "```\n",
      "Line 1: <statement>\n",
      "Line 2: <statement>\n",
      "...\n",
      "```\n",
      "\n",
      "For each line, show:\n",
      "- The line number\n",
      "- The line content\n",
      "- The current state of the variables (as a dictionary)\n",
      "- The next line to execute\n",
      "\n",
      "Make sure to show the recursive calls and their stack frames.\n",
      "\n",
      "We are going to trace the execution of the given code step by step.\n",
      "\n",
      " The code:\n",
      "   def fibonacci(n):\n",
      "        if n <= 1:\n",
      "            return n\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "   result = fibonacci(4)\n",
      "   print(result)\n",
      "\n",
      " We'll break it down:\n",
      "\n",
      " Step 1: Define the function `fibonacci` (this is not a line of execution in the main flow, but we'll note it)\n",
      " Step 2: Call `fibonacci(4)`\n",
      "\n",
      " We are to show the execution trace for the main flow.\n",
      "\n",
      " Let's write the trace in the required format.\n",
      "\n",
      " Note: We have to show the stack frames for recursion.\n",
      "\n",
      " We'll start from the main call.\n",
      "\n",
      " We'll use a trace that shows:\n",
      "   Line 1: ... (the first line of the function body? but note: the function definition is not executed as a line of code in the trace for the main flow)\n",
      "\n",
      " However, the problem says: \"Provide a detailed execution trace showing variable states at each line\"\n",
      "\n",
      " We are to show the lines of the code that are executed.\n",
      "\n",
      " Let's list the lines of the code (with the function body):\n",
      "\n",
      "   Line 1: def fibonacci(n):\n",
      "   Line 2:     if n <= 1:\n",
      "   Line 3:         return n\n",
      "   Line 4:     return fibonacci(n-1) + fibonacci(n-2)\n",
      "   Line 5: result = fibonacci(4)\n",
      "   Line 6: print(result)\n",
      "\n",
      " But note: the problem says \"step by step\", so we have to show the execution of the function calls.\n",
      "\n",
      " We'll start from the main call to `fibonacci(4)`.\n",
      "\n",
      " However, the problem says: \"the following format\" for each line.\n",
      "\n",
      " We are to show the line number (as in the code we are given) and the state.\n",
      "\n",
      " Let's do it step by step.\n",
      "\n",
      " Important: We have to show the stack frames. So when we enter a recursive call, we create a new frame.\n",
      "\n",
      " We'll use a dictionary for the current state of variables (the local variables in the current frame).\n",
      "\n",
      " We'll start at the top level.\n",
      "\n",
      " Step 0: Before the function call, we have:\n",
      "   result = None (but we haven't assigned it yet)\n",
      "\n",
      " But the problem says: \"the current state of the variables\" at each line.\n",
      "\n",
      " We'll break the trace into the lines of the code.\n",
      "\n",
      " We are going to simulate the execution.\n",
      "\n",
      " Let me write the trace:\n",
      "\n",
      " We start by defining the function (but that's not a line of execution in the main flow). Then we do:\n",
      "\n",
      "   Line 5: result = fibonacci(4)\n",
      "\n",
      " So we start from there.\n",
      "\n",
      " However, the problem says: \"the following format\" for each line.\n",
      "\n",
      " We'll show the lines of the code as they are executed.\n",
      "\n",
      " We have to be careful: the function definition is not executed as a line of code in the main flow? Actually, the problem says \"the code\" and then the function definition and the rest.\n",
      "\n",
      " But the trace should start from the first line of the main code that runs (which is the function call).\n",
      "\n",
      " Let's write the trace:\n",
      "\n",
      " We'll use a list of steps.\n",
      "\n",
      " Step 1: We are at the top level, and we are about to execute the line: `result = fibonacci(4)`\n",
      "\n",
      " But note: the problem says \"step by step\", so we have to show the entire recursion.\n",
      "\n",
      " We'll do:\n",
      "\n",
      "   Line 5: result = fibonacci(4)\n",
      "\n",
      " At this point, the current frame (the main frame) has:\n",
      "   result = None (because we haven't assigned it yet)\n",
      "\n",
      " However, the problem says: \"the current state of the variables (as a dictionary)\"\n",
      "\n",
      " We'll show the state of the variables in the current frame (the top-level frame) at the moment we are about to execute the line.\n",
      "\n",
      " But note: the function call `fibonacci(4)` will create a new frame.\n",
      "\n",
      " We have to show the stack.\n",
      "\n",
      " Let me try to write the trace in the required format.\n",
      "\n",
      " We'll do:\n",
      "\n",
      "   Line 5: result = fibonacci(4)\n",
      "        State: {'result': None}   (because we haven't assigned it yet, and the function hasn't been called)\n",
      "        Next line: 6 (the print)\n",
      "\n",
      " But then we enter the function.\n",
      "\n",
      " However, the problem says: \"show the recursive calls and their stack frames\".\n",
      "\n",
      " So we have to show the function calls.\n",
      "\n",
      " Let me try to write the entire trace.\n",
      "\n",
      " We'll break it down:\n",
      "\n",
      " 1. Start: main frame (top-level) with no variables (or with the global variables? but the problem says \"current state of\n",
      "\n",
      "‚úÖ Model testing complete!\n"
     ]
    }
   ],
   "source": [
    "# Quick usage example\n",
    "print(\"üöÄ Testing the trained model...\")\n",
    "\n",
    "def test_trained_model(code: str) -> str:\n",
    "    \"\"\"Test the trained model on a code example.\"\"\"\n",
    "    prompt = f\"\"\"Analyze this code and predict its execution trace step by step:\n",
    "\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Provide a detailed execution trace showing variable states at each line.\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(trainer.model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][len(inputs.input_ids[0]):], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Test example\n",
    "test_code = \"\"\"def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "result = fibonacci(4)\n",
    "print(result)\"\"\"\n",
    "\n",
    "print(\"\\nüß™ Test Code:\")\n",
    "print(test_code)\n",
    "\n",
    "print(\"\\nü§ñ Model Response:\")\n",
    "response = test_trained_model(test_code)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n‚úÖ Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_section"
   },
   "source": [
    "## üìã Training Summary\n",
    "\n",
    "### üéØ Recipe #6: Execution-Aware World Model Thinking LoRA\n",
    "\n",
    "**Objective**: Add execution awareness to Qwen3-4B-Thinking-2507 using CWM-inspired techniques\n",
    "\n",
    "**Key Innovation**: Combined thinking-based reasoning with real execution traces for ground truth learning\n",
    "\n",
    "**Training Approach**:\n",
    "1. **Hybrid Data Generation**: Magpie-style code generation + real execution tracing\n",
    "2. **DPO Training**: Preference learning between predicted and actual execution traces\n",
    "3. **Self-Supervised**: No manual annotation required\n",
    "\n",
    "**Capabilities Added**:\n",
    "- Step-by-step execution prediction\n",
    "- Variable state tracking\n",
    "- Debugging with execution awareness\n",
    "- Understanding of program behavior\n",
    "\n",
    "### üìä Results\n",
    "- Model successfully trained with execution awareness\n",
    "- Evaluation shows improved understanding of code behavior\n",
    "- Ready for integration with existing Ellora recipes\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook implements Ellora Recipe #6, bringing CWM's execution awareness to smaller, more efficient models through LoRA adaptation.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
